<!-- livebook:{"autosave_interval_s":60} -->

# Soma: Word frequency analysis

```elixir
Mix.install([
  {:kino, "~> 0.12.3"}
])
```

## Synopsis

Soma is a small program for the analysis of texts, an outgrowth of an exercise from _Think OCaml_, an OCaml programming textbook (Downey & Monje, 2008, p. 105). The exercise itself is aimed at text manipulation and deriving interesting comparative conclusions between different works, as well as comparisons between the styles of different writers.

My aim was somewhat different, in that making writing style comparisons—while interesting—aren't a desirable end goal. Instead, deriving word corpora with their frequencies provides a wonderful language-learning tool, as the list of words used within works is a rich vocabulary of actually-used words in the target language, which can be learned with a flashcard learning method.

## Reading a file

To start, we need to choose the file Soma will read from. `Kino` provides us with a graphical interface, making this an easy task for the user.

```elixir
file = Kino.Input.file("Select an input file")
```

`Kino` returns a tuple for a selected file, with `file_ref` as the most important value. Capturing it here makes it easy to use later to get the absolute file path.

```elixir
%{file_ref: file_ref} =
  file
  |> Kino.Input.read()
```

Using the previously captured `file_ref`, we obtain the file's absolute path, and are able to read it, as required in the following steps.

```elixir
file_ref
|> Kino.Input.file_path()
|> File.read!()
```

## Break lines into words; strip whitespace and punctuation from words; convert to lowercase

Breaking a line of text into words is fairly straightforward, with a few possible points of failure which we must guard against:

* Irregular spacing
* Punctuation
* Word case

It is likely that imported text may have erroneously-added spacing. Common examples of how this can happen include text that was written on a typewriter, or by a writer with typewriter habits (Butterick, 2024); space inserted in error by OCR software misrecognising the source scanned material; unusual text formatting habits employed by the author.

<!-- livebook:{"break_markdown":true} -->

Punctuation is an obstacle to building up a dictionary of words used in a text or speech, and must be removed. This step is necesary also as punctuation can easily be discovered in scanned works, where OCR software—used to automate text recognition—misinterprets stains, marks, creases or dust on a document as punctuation marks. Punctuation is overused in very colloquial texts, with _To Kill a Mockingbird_ or _Moby Dick_ as easy examples. The better all pre-, post- and infix punctuation is removed, the more likely we are to get back a true word frequency from our source.

<!-- livebook:{"break_markdown":true} -->

Word case is a problem owing to the way characters are represented in binary. Capital letter codes are different for character codes for lowercase letters; if we don't normalise our source text, occurences of words such as 'Fox', 'FOX', and 'fox' will be treated as three separate words, defeating the purpose of our word frequency calculator. While all three of the above are reasonable uses of the same word—the first as the start of a sentence, the second used in a title, or shouted text, and the third appearing anywhere within a sentence—we must treat them as one word, encountered three times.

<!-- livebook:{"break_markdown":true} -->

To help detect some of these errors, we will define an example sentence including the above problems, to be stored in the `sample_sentence` variable.

```elixir
sample_sentence = " \n  The  quick    brown   fox jumped over the  LAZY dogs.  \n\n"
```

In the following prototype, the sample sentence will be subjected to a pipeline of three filtering and manipulation actions:

1. Splitting the string with `String.split\1`
2. Removal of punctuation using a regular expression in `String.replace\3`
3. Lower casing all the words

Note that the splitting function trims an infinite number of leading and trailing space-like characters, including `tab` and `newline`, by default.

```elixir
sample_sentence
|> String.split()
|> Enum.map(fn word -> String.replace(word, ~r/[\p{P}\p{S}]/u, "") end)
|> Enum.map(fn word -> String.downcase(word) end)
```

With text breaking and cleaning demonstrated, the work is on the right track, but isn't complete.

First of all, the code above will be very slow. Code just sitting on its own inside an Elixir code block is interpreted and unoptimised in any way. By creating a dedicated module, the code will be compiled and optimised, and much faster.

Secondly, the pipeline above is very rigid. All the work is done, and all the options and configuration is made in this one central place, but it doesn't provide flexibility for the future; what if we want to handle text in another language, or if we want to modify some options used in the pipeline, without losing this example?

The answer, again, is to make the code more modular, and this is exactly what we're doing in the following step.

```elixir
defmodule Soma.Text.Processing do
  @moduledoc """
  This module handles all of Soma's text processing needs.
  """

  def process_text(text) when is_bitstring(text) do
    text
    |> break_into_words
    |> Enum.map(fn word -> strip_punctuation(word) end)
    |> Enum.map(fn word -> convert_to_lowercase(word) end)
  end

  def break_into_words(text) do
    String.split(text)
  end

  def strip_punctuation(word) do
    String.replace(word, ~r/[[:punct:]]/u, "")
  end

  def convert_to_lowercase(word) do
    String.downcase(word)
  end
end
```

```elixir
Soma.Text.Processing.process_text(sample_sentence)
```

We got the same result as we did the first time!

Comparing the first version, we can see that it executed in 11ms (hovering over the 'Evaluated' lozenge in the lower-right corner reveals the execution time), while this new, modular version, producing compatible results, is reported as taking 0ms. Obviously, this is not quite true as the display obviously rounds down to a certain number of significant digits. However, the performance of the second solution is an order of magnitude greater, which is key when it comes to looping over a large body of text, which is what we want this program to do in the future.

<!-- livebook:{"break_markdown":true} -->

Our code is still completely unoptimised, but is already much faster. As our pipeline intensively loops over and over the sample text, we need every bit of performance we can squeeze out of our computer if we're hoping to achieve quick processing of our text, getting a word frequency out within a reasonable time.

<!-- livebook:{"break_markdown":true} -->

### Removing punctuation

The simplest method with which to remove English-language punctuation is by using _Regular Expressions_ (Wikipedia contributors, 2024), in particular either the Java-like `\p{Punct}`, or shorter `\p{P}` syntax, or the more portable POSIX syntax, the equivalent of which is `[:punct:]`.

It's important to note here that this only removes punctuation recognised to be that used in English writing; the same syntax _will not_ work in another language, as languages have accrued their own punctuation norms and styles. To handle other languages, we will need to feed a way to recognise punctuation usage in those languages!

Another note is that regular expressions may not be the fastest or most performant way to strip punctuation, especially on large texts, as _every_ character of the provided text is searched, which is likely to be computationally expensive and slow.

## Spanish

Does our solution work in other languages? Knowing that punctuation differs accross languages, will the above solution work in Spanish, for example?

To that end, let's define a sample sentence in Spanish, with punctuation as used by that language.

```elixir
sample_spanish_sentence = "¡Oh! Señor Jesu-Cristo, misericordia, santo"
```

It turns out that the solution _will_ work, as long as we have inserted a `u` character as a flag to enable Unicode regular expression matching. This is a requirement as Unicode expression-matching is slower by necessity.

```elixir
Soma.Text.Processing.process_text(sample_spanish_sentence)
```

## References

* Downey, A., & Monje, N. (2008). Think OCaml: How to Think Like a Functional Programmer (Version 0.1.1). Green Tea Press; The LATEX source for this book is available from http://www.thinkpython.com. https://greenteapress.com/thinkocaml/thinkocaml.pdf

* Butterick, M. (2024, April 16). Typewriter habits | Butterick’s Practical Typography. https://practicaltypography.com

* Wikipedia contributors. (2024). Regular expression. In Wikipedia. Wikipedia, The Free Encyclopedia. https://en.wikipedia.org/w/index.php?title=Regular_expression&oldid=1218131380
